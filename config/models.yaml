# =============================================================================
# NVIDIA NIM Model Configuration for Cyber-Red
# =============================================================================
# Priority: Kimi K2 Thinking > DeepSeek v3.2 > MiniMax M2
# These are the most capable models for offensive security reasoning.
# =============================================================================

# -----------------------------------------------------------------------------
# BRAIN - Strategic Reasoning Models
# -----------------------------------------------------------------------------
brain:
  # Primary strategic reasoning
  # Kimi K2 Instruct: Working version (kimi-k2-thinking returns None)
  architect: "moonshotai/kimi-k2-instruct"

  
  # Deep analysis and multi-step planning
  # DeepSeek v3.2: 685B reasoning LLM, sparse attention, agentic tools
  strategist: "deepseek-ai/deepseek-v3.2"
  
  # Evasion and stealth optimization
  # MiniMax M2: 230B MoE, 10B active, optimized for tool-use/agents
  ghost: "minimaxai/minimax-m2"

# -----------------------------------------------------------------------------
# GOVERNANCE - Safety and Moderation
# -----------------------------------------------------------------------------
governance:
  # Fast safety check for command approval
  # Llama 3.3 70B: Latest Meta model, faster than 3.1
  critic: "meta/llama-3.3-70b-instruct"
  
  # Intent parsing from natural language (speed-optimized)
  dispatcher: "meta/llama-3.3-70b-instruct"

# -----------------------------------------------------------------------------
# CODE GENERATION - Exploit and Payload Creation
# -----------------------------------------------------------------------------
code_generation:
  # Primary exploit/payload generation
  # DeepSeek v3.2 excels at code reasoning
  engineer: "deepseek-ai/deepseek-v3.2"
  
  # Alternative for pure code tasks
  coder: "qwen/qwen2.5-coder-32b-instruct"

# -----------------------------------------------------------------------------
# MODEL PARAMETERS
# -----------------------------------------------------------------------------
parameters:
  # Temperature settings per role
  architect:
    temperature: 0.7
    max_tokens: 2000  # More tokens for <thinking> tags
    
  strategist:
    temperature: 0.5
    max_tokens: 1500
    
  ghost:
    temperature: 0.5
    max_tokens: 500
    
  critic:
    temperature: 0.1  # Very deterministic for safety
    max_tokens: 200
    
  dispatcher:
    temperature: 0.1
    max_tokens: 150
    
  engineer:
    temperature: 0.2  # Low temp for precise code
    max_tokens: 1000

# -----------------------------------------------------------------------------
# FALLBACK MODELS
# In case primary models are unavailable or rate-limited
# -----------------------------------------------------------------------------
fallback:
  architect: "deepseek-ai/deepseek-v3.1"
  strategist: "moonshotai/kimi-k2-instruct"
  ghost: "meta/llama-3.3-70b-instruct"
  critic: "nvidia/llama-3.1-nemotron-70b-instruct"
  engineer: "mistralai/devstral-2-123b-instruct-2512"
